{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e16b71-e6a0-4053-8595-c90e34ec253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Imports\n",
    "# ===============================s\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as pdt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import gc\n",
    "from xgboost.callback import EarlyStopping as XGBEarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e12c49-dc6d-4ed7-8c8a-0e0bf0878a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XGBOOST MODEL TRAINING WITH OUTSTANDING RESULTS THROUGH THE ETL-PIPELINE DATASET\n",
    "--------------------------------------------------------------------------------\n",
    "This notebook/script trains, calibrates, and evaluates a binary XGBoost model on\n",
    "the match-level dataset produced by the ETL pipeline. It follows leak-safe\n",
    "preprocessing, yearly cross-validation grouped by match `id`, isotonic\n",
    "probability calibration, and out-of-sample testing (2023–2025). Results are\n",
    "strong across eras and tournament tiers.\n",
    "\n",
    "Key takeaways (summarized from the printed runs in this notebook):\n",
    "- Yearly CV (2000–2025, 26 folds; grouping by `id`):\n",
    "    * Mean AUC ≈ 0.96375\n",
    "    * Mean LogLoss ≈ 0.22444\n",
    "    * Mean Brier ≈ 0.07242\n",
    "    * Mean Accuracy ≈ 0.89002\n",
    "- OOF calibration (2000–2022, isotonic):\n",
    "    * AUC ≈ 0.9719 (stable pre/post calibration)\n",
    "    * LogLoss: 0.2113 → 0.2099 after calibration\n",
    "    * Optimal decision threshold by costs: ~0.4959\n",
    "- Hold-out 2023–2025 (using calibrated probabilities):\n",
    "    * Mean AUC ≈ 0.9145, LogLoss ≈ 0.3794, Brier ≈ 0.1198\n",
    "    * Mean Acc@0.5 ≈ 0.8205, Mean Acc@0.496 ≈ 0.8206\n",
    "- Hold-out by tournament tier (2023–2025, Acc@0.5):\n",
    "    * Challenger-like:  AUC ≈ 0.9200 | LogLoss ≈ 0.3696 | Brier ≈ 0.1177 | Acc ≈ 0.8228\n",
    "    * ATP Tour:         AUC ≈ 0.9142 | LogLoss ≈ 0.3738 | Brier ≈ 0.1207 | Acc ≈ 0.8187\n",
    "    * Masters 1000:     AUC ≈ 0.8971 | LogLoss ≈ 0.4313 | Brier ≈ 0.1352 | Acc ≈ 0.7997\n",
    "    * Grand Slams:      AUC ≈ 0.9324 | LogLoss ≈ 0.3682 | Brier ≈ 0.1091 | Acc ≈ 0.8443\n",
    "\n",
    "Design choices that drive these results:\n",
    "- Strict no-leakage preprocessing with sklearn pipelines and `ColumnTransformer`.\n",
    "- Categorical encoding via OHE into sparse matrices; numeric features coerced to float32.\n",
    "- XGBoost `hist` tree method for speed; careful regularization & early stopping.\n",
    "- “Best iteration” selected on 2022 (hold-out for early stopping), then retraining on ≤2022.\n",
    "- Isotonic calibration on OOF (2000–2022), applied to 2023–2025 test.\n",
    "- Efficient batch transforms to minimize overhead.\n",
    "\n",
    "Usage:\n",
    "- Set `csv_path` to the dataset CSV exported from the ETL pipeline.\n",
    "- Run cells in order or execute as a script (they are linear and independent).\n",
    "- Dependencies: numpy, pandas, scikit-learn, scipy, xgboost.\n",
    "\n",
    "Notes:\n",
    "- Metrics above summarize typical outcomes obtained from prior runs on this notebook.\n",
    "- If your dataset slice differs, numbers may vary slightly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37caaffd-d146-4fd7-9083-7a7185ab8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1) Carga ====\n",
    "csv_path = \"\"\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, low_memory=False, na_values=[\"\", \"NA\", \"NaN\"])\n",
    "    if \"tournament_start_dtm\" in df.columns:\n",
    "        df[\"tournament_start_dtm\"] = pd.to_datetime(df[\"tournament_start_dtm\"], errors=\"coerce\")\n",
    "except FileNotFoundError:\n",
    "    raise SystemExit(f\"No se encontró el archivo: {csv_path}\")\n",
    "\n",
    "# Limpieza de columnas índice exportadas\n",
    "idx_cols = [c for c in df.columns if c.startswith(\"Unnamed\")]\n",
    "if \"V1\" in df.columns:\n",
    "    idx_cols.append(\"V1\")\n",
    "if idx_cols:\n",
    "    df = df.drop(columns=idx_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a803c5f2-3d0e-4ea3-9935-d3acfb950454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 2) Saneos mínimos ====\n",
    "for c in [c for c in df.columns if c.startswith(\"log_\")]:\n",
    "    df[c] = df[c].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1a316b-5db1-4125-b268-179dc23b2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 3) Filtro temporal ====\n",
    "if \"year\" not in df.columns:\n",
    "    raise ValueError(\"No se encontró la columna 'year'.\")\n",
    "df = df[df[\"year\"] > 1999].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e18530d-3400-4356-8a39-594e09082e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"match_result\", \"id\"]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Falta la columna obligatoria '{col}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a8155b5-4ad2-430b-8132-72276dc32c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 5) y, X base ====\n",
    "# y binaria ya en el CSV (0/1). Si fuese 'win'/'loss': y = (df[\"match_result\"] == \"win\").astype(int)\n",
    "y = df[\"match_result\"].astype(int).values\n",
    "X = df.drop(columns=[\"match_result\"]).copy()   # <- IMPORTANTÍSIMO: sacar el target de las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad26662-6fb8-459d-ab73-3eda8cf9b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 6) Categóricas (las tuyas) ====\n",
    "cat_cols = [\n",
    "    \"surface\",\n",
    "    \"player_handedness\",\"player_backhand\",\n",
    "    \"opponent_handedness\",\"opponent_backhand\",\n",
    "    \"indoor_outdoor\",\n",
    "    \"player_rank_trend_4w_cat\",\"player_rank_trend_12w_cat\",\n",
    "    \"opponent_rank_trend_4w_cat\",\"opponent_rank_trend_12w_cat\",\n",
    "    \"player_win_prob_diff_general_vs_surface_cat\",\n",
    "    \"opponent_win_prob_diff_general_vs_surface_cat\",\n",
    "    \"best_of\",\n",
    "    \"player_home\",\"opponent_home\",\n",
    "    \"player_favourite_surface\",\"opponent_favourite_surface\",\n",
    "    \"player_good_form_5\",\"player_good_form_10\",\n",
    "    \"opponent_good_form_5\",\"opponent_good_form_10\",\n",
    "    \"player_won_previous_tournament\",\"opponent_won_previous_tournament\",\n",
    "    \"player_back_to_back_week\",\"player_two_weeks_gap\",\"player_long_rest\",\n",
    "    \"player_country_changed\",\"player_surface_changed\",\"player_indoor_changed\",\n",
    "    \"player_continent_changed\",\"player_red_eye_risk\",\n",
    "    \"opponent_back_to_back_week\",\"opponent_two_weeks_gap\",\"opponent_long_rest\",\n",
    "    \"opponent_country_changed\",\"opponent_surface_changed\",\n",
    "    \"opponent_indoor_changed\",\"opponent_continent_changed\",\n",
    "    \"opponent_red_eye_risk\",\n",
    "    \"has_player_h2h_surface\",\"has_player_h2h_full\"\n",
    "]\n",
    "cat_cols = [c for c in cat_cols if c in X.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5f2492-26c3-413a-bd44-40ff3421e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 8) Tipos: categóricas a string→object (con np.nan), numéricas a float32 ====\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype(\"string\")\n",
    "X[cat_cols] = X[cat_cols].astype(object)\n",
    "X[cat_cols] = X[cat_cols].mask(X[cat_cols].isna(), np.nan)\n",
    "\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "for c in num_cols:\n",
    "    if not pdt.is_numeric_dtype(X[c]):\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    if pdt.is_numeric_dtype(X[c]):\n",
    "        X[c] = X[c].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6637e976-d09e-48c9-85b8-cdc0b014caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder en *sparse*\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
    "    (\"ohe\", ohe)\n",
    "])\n",
    "\n",
    "# Fuerza salida sparse si hay algo sparse\n",
    "pre_template = ColumnTransformer(\n",
    "    transformers=[(\"cat\", cat_transformer, cat_cols)],\n",
    "    remainder=\"passthrough\",\n",
    "    sparse_threshold=1.0   # <- clave para que no densifique al concatenar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8d3c88-879a-4125-bf1d-b51d658e7f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV por año con garantía por 'id': 26 folds — años: [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
      "[0]\tvalid-logloss:0.67906\tvalid-auc:0.74039\n",
      "[200]\tvalid-logloss:0.45785\tvalid-auc:0.83105\n",
      "[217]\tvalid-logloss:0.45805\tvalid-auc:0.83150\n",
      "Fold 1 | year 2000 | AUC=0.8308  LogLoss=0.4567  Brier=0.1598  Acc=0.7154  best_iter=167\n",
      "[0]\tvalid-logloss:0.66611\tvalid-auc:0.93306\n",
      "[200]\tvalid-logloss:0.17782\tvalid-auc:0.98144\n",
      "[400]\tvalid-logloss:0.17209\tvalid-auc:0.98228\n",
      "[600]\tvalid-logloss:0.16925\tvalid-auc:0.98260\n",
      "[800]\tvalid-logloss:0.16763\tvalid-auc:0.98272\n",
      "[910]\tvalid-logloss:0.16763\tvalid-auc:0.98264\n",
      "Fold 2 | year 2001 | AUC=0.9827  LogLoss=0.1674  Brier=0.0531  Acc=0.9190  best_iter=860\n",
      "[0]\tvalid-logloss:0.66632\tvalid-auc:0.92722\n",
      "[200]\tvalid-logloss:0.17819\tvalid-auc:0.98159\n",
      "[400]\tvalid-logloss:0.17160\tvalid-auc:0.98247\n",
      "[600]\tvalid-logloss:0.16799\tvalid-auc:0.98299\n",
      "[800]\tvalid-logloss:0.16657\tvalid-auc:0.98317\n",
      "[1000]\tvalid-logloss:0.16522\tvalid-auc:0.98337\n",
      "[1200]\tvalid-logloss:0.16460\tvalid-auc:0.98337\n",
      "[1400]\tvalid-logloss:0.16364\tvalid-auc:0.98349\n",
      "[1513]\tvalid-logloss:0.16356\tvalid-auc:0.98348\n",
      "Fold 3 | year 2002 | AUC=0.9835  LogLoss=0.1634  Brier=0.0521  Acc=0.9203  best_iter=1464\n",
      "[0]\tvalid-logloss:0.66620\tvalid-auc:0.92799\n",
      "[200]\tvalid-logloss:0.17181\tvalid-auc:0.98360\n",
      "[400]\tvalid-logloss:0.16508\tvalid-auc:0.98428\n",
      "[600]\tvalid-logloss:0.16208\tvalid-auc:0.98463\n",
      "[800]\tvalid-logloss:0.16042\tvalid-auc:0.98475\n",
      "[1000]\tvalid-logloss:0.15933\tvalid-auc:0.98484\n",
      "[1200]\tvalid-logloss:0.15843\tvalid-auc:0.98489\n",
      "[1400]\tvalid-logloss:0.15788\tvalid-auc:0.98491\n",
      "[1479]\tvalid-logloss:0.15777\tvalid-auc:0.98490\n",
      "Fold 4 | year 2003 | AUC=0.9849  LogLoss=0.1577  Brier=0.0498  Acc=0.9262  best_iter=1430\n",
      "[0]\tvalid-logloss:0.66627\tvalid-auc:0.92627\n",
      "[200]\tvalid-logloss:0.17369\tvalid-auc:0.98249\n",
      "[400]\tvalid-logloss:0.16734\tvalid-auc:0.98327\n",
      "[600]\tvalid-logloss:0.16459\tvalid-auc:0.98366\n",
      "[800]\tvalid-logloss:0.16273\tvalid-auc:0.98388\n",
      "[935]\tvalid-logloss:0.16207\tvalid-auc:0.98393\n",
      "Fold 5 | year 2004 | AUC=0.9840  LogLoss=0.1620  Brier=0.0512  Acc=0.9227  best_iter=886\n",
      "[0]\tvalid-logloss:0.66526\tvalid-auc:0.93884\n",
      "[200]\tvalid-logloss:0.15974\tvalid-auc:0.98538\n",
      "[400]\tvalid-logloss:0.15321\tvalid-auc:0.98621\n",
      "[600]\tvalid-logloss:0.14970\tvalid-auc:0.98668\n",
      "[800]\tvalid-logloss:0.14849\tvalid-auc:0.98684\n",
      "[1000]\tvalid-logloss:0.14737\tvalid-auc:0.98693\n",
      "[1075]\tvalid-logloss:0.14744\tvalid-auc:0.98690\n",
      "Fold 6 | year 2005 | AUC=0.9869  LogLoss=0.1473  Brier=0.0463  Acc=0.9304  best_iter=1026\n",
      "[0]\tvalid-logloss:0.66620\tvalid-auc:0.92573\n",
      "[200]\tvalid-logloss:0.17594\tvalid-auc:0.98225\n",
      "[400]\tvalid-logloss:0.16966\tvalid-auc:0.98315\n",
      "[600]\tvalid-logloss:0.16677\tvalid-auc:0.98354\n",
      "[800]\tvalid-logloss:0.16463\tvalid-auc:0.98382\n",
      "[1000]\tvalid-logloss:0.16389\tvalid-auc:0.98390\n",
      "[1200]\tvalid-logloss:0.16289\tvalid-auc:0.98402\n",
      "[1400]\tvalid-logloss:0.16216\tvalid-auc:0.98411\n",
      "[1600]\tvalid-logloss:0.16153\tvalid-auc:0.98419\n",
      "[1800]\tvalid-logloss:0.16124\tvalid-auc:0.98424\n",
      "[1879]\tvalid-logloss:0.16103\tvalid-auc:0.98426\n",
      "Fold 7 | year 2006 | AUC=0.9843  LogLoss=0.1610  Brier=0.0506  Acc=0.9257  best_iter=1829\n",
      "[0]\tvalid-logloss:0.66731\tvalid-auc:0.90534\n",
      "[200]\tvalid-logloss:0.20692\tvalid-auc:0.97503\n",
      "[400]\tvalid-logloss:0.20021\tvalid-auc:0.97609\n",
      "[600]\tvalid-logloss:0.19723\tvalid-auc:0.97659\n",
      "[800]\tvalid-logloss:0.19536\tvalid-auc:0.97690\n",
      "[1000]\tvalid-logloss:0.19375\tvalid-auc:0.97715\n",
      "[1200]\tvalid-logloss:0.19309\tvalid-auc:0.97725\n",
      "[1400]\tvalid-logloss:0.19243\tvalid-auc:0.97734\n",
      "[1600]\tvalid-logloss:0.19170\tvalid-auc:0.97748\n",
      "[1762]\tvalid-logloss:0.19163\tvalid-auc:0.97747\n",
      "Fold 8 | year 2007 | AUC=0.9775  LogLoss=0.1916  Brier=0.0606  Acc=0.9121  best_iter=1713\n",
      "[0]\tvalid-logloss:0.66813\tvalid-auc:0.89264\n",
      "[200]\tvalid-logloss:0.23154\tvalid-auc:0.96726\n",
      "[400]\tvalid-logloss:0.22375\tvalid-auc:0.96912\n",
      "[600]\tvalid-logloss:0.22052\tvalid-auc:0.96992\n",
      "[800]\tvalid-logloss:0.21913\tvalid-auc:0.97025\n",
      "[1000]\tvalid-logloss:0.21836\tvalid-auc:0.97042\n",
      "[1200]\tvalid-logloss:0.21769\tvalid-auc:0.97055\n",
      "[1400]\tvalid-logloss:0.21708\tvalid-auc:0.97068\n",
      "[1537]\tvalid-logloss:0.21683\tvalid-auc:0.97073\n",
      "Fold 9 | year 2008 | AUC=0.9708  LogLoss=0.2166  Brier=0.0690  Acc=0.8982  best_iter=1487\n",
      "[0]\tvalid-logloss:0.66817\tvalid-auc:0.89832\n",
      "[200]\tvalid-logloss:0.22193\tvalid-auc:0.97035\n",
      "[400]\tvalid-logloss:0.21456\tvalid-auc:0.97192\n",
      "[600]\tvalid-logloss:0.21119\tvalid-auc:0.97263\n",
      "[800]\tvalid-logloss:0.20890\tvalid-auc:0.97310\n",
      "[1000]\tvalid-logloss:0.20792\tvalid-auc:0.97327\n",
      "[1010]\tvalid-logloss:0.20786\tvalid-auc:0.97329\n",
      "Fold 10 | year 2009 | AUC=0.9733  LogLoss=0.2078  Brier=0.0663  Acc=0.9005  best_iter=960\n",
      "[0]\tvalid-logloss:0.66878\tvalid-auc:0.88828\n",
      "[200]\tvalid-logloss:0.23662\tvalid-auc:0.96578\n",
      "[400]\tvalid-logloss:0.22835\tvalid-auc:0.96777\n",
      "[600]\tvalid-logloss:0.22450\tvalid-auc:0.96864\n",
      "[800]\tvalid-logloss:0.22216\tvalid-auc:0.96918\n",
      "[1000]\tvalid-logloss:0.22077\tvalid-auc:0.96949\n",
      "[1200]\tvalid-logloss:0.21956\tvalid-auc:0.96977\n",
      "[1400]\tvalid-logloss:0.21902\tvalid-auc:0.96984\n",
      "[1600]\tvalid-logloss:0.21868\tvalid-auc:0.96989\n",
      "[1800]\tvalid-logloss:0.21829\tvalid-auc:0.96996\n",
      "[1942]\tvalid-logloss:0.21815\tvalid-auc:0.96998\n",
      "Fold 11 | year 2010 | AUC=0.9700  LogLoss=0.2181  Brier=0.0703  Acc=0.8940  best_iter=1893\n",
      "[0]\tvalid-logloss:0.66783\tvalid-auc:0.90056\n",
      "[200]\tvalid-logloss:0.22327\tvalid-auc:0.96990\n",
      "[400]\tvalid-logloss:0.21596\tvalid-auc:0.97147\n",
      "[600]\tvalid-logloss:0.21262\tvalid-auc:0.97222\n",
      "[800]\tvalid-logloss:0.21075\tvalid-auc:0.97263\n",
      "[1000]\tvalid-logloss:0.20903\tvalid-auc:0.97298\n",
      "[1200]\tvalid-logloss:0.20816\tvalid-auc:0.97315\n",
      "[1302]\tvalid-logloss:0.20787\tvalid-auc:0.97322\n",
      "Fold 12 | year 2011 | AUC=0.9733  LogLoss=0.2078  Brier=0.0658  Acc=0.9010  best_iter=1252\n",
      "[0]\tvalid-logloss:0.66776\tvalid-auc:0.89989\n",
      "[200]\tvalid-logloss:0.22594\tvalid-auc:0.96866\n",
      "[400]\tvalid-logloss:0.21816\tvalid-auc:0.97055\n",
      "[600]\tvalid-logloss:0.21440\tvalid-auc:0.97145\n",
      "[800]\tvalid-logloss:0.21193\tvalid-auc:0.97200\n",
      "[1000]\tvalid-logloss:0.21060\tvalid-auc:0.97231\n",
      "[1200]\tvalid-logloss:0.20966\tvalid-auc:0.97249\n",
      "[1400]\tvalid-logloss:0.20899\tvalid-auc:0.97263\n",
      "[1600]\tvalid-logloss:0.20855\tvalid-auc:0.97272\n",
      "[1759]\tvalid-logloss:0.20838\tvalid-auc:0.97275\n",
      "Fold 13 | year 2012 | AUC=0.9728  LogLoss=0.2084  Brier=0.0668  Acc=0.9001  best_iter=1709\n",
      "[0]\tvalid-logloss:0.66787\tvalid-auc:0.90116\n",
      "[200]\tvalid-logloss:0.22757\tvalid-auc:0.96850\n",
      "[400]\tvalid-logloss:0.22002\tvalid-auc:0.97026\n",
      "[600]\tvalid-logloss:0.21719\tvalid-auc:0.97092\n",
      "[800]\tvalid-logloss:0.21541\tvalid-auc:0.97130\n",
      "[1000]\tvalid-logloss:0.21418\tvalid-auc:0.97157\n",
      "[1200]\tvalid-logloss:0.21290\tvalid-auc:0.97184\n",
      "[1400]\tvalid-logloss:0.21217\tvalid-auc:0.97199\n",
      "[1512]\tvalid-logloss:0.21183\tvalid-auc:0.97206\n",
      "Fold 14 | year 2013 | AUC=0.9721  LogLoss=0.2118  Brier=0.0674  Acc=0.8998  best_iter=1463\n",
      "[0]\tvalid-logloss:0.66777\tvalid-auc:0.90125\n",
      "[200]\tvalid-logloss:0.22498\tvalid-auc:0.96940\n",
      "[400]\tvalid-logloss:0.21779\tvalid-auc:0.97116\n",
      "[600]\tvalid-logloss:0.21407\tvalid-auc:0.97206\n",
      "[800]\tvalid-logloss:0.21226\tvalid-auc:0.97247\n",
      "[1000]\tvalid-logloss:0.21093\tvalid-auc:0.97275\n",
      "[1200]\tvalid-logloss:0.20997\tvalid-auc:0.97296\n",
      "[1400]\tvalid-logloss:0.20950\tvalid-auc:0.97305\n",
      "[1600]\tvalid-logloss:0.20882\tvalid-auc:0.97320\n",
      "[1800]\tvalid-logloss:0.20801\tvalid-auc:0.97336\n",
      "[1999]\tvalid-logloss:0.20771\tvalid-auc:0.97341\n",
      "Fold 15 | year 2014 | AUC=0.9734  LogLoss=0.2077  Brier=0.0657  Acc=0.9057  best_iter=1988\n",
      "[0]\tvalid-logloss:0.66860\tvalid-auc:0.89496\n",
      "[200]\tvalid-logloss:0.23095\tvalid-auc:0.96795\n",
      "[400]\tvalid-logloss:0.22318\tvalid-auc:0.96969\n",
      "[600]\tvalid-logloss:0.21914\tvalid-auc:0.97062\n",
      "[800]\tvalid-logloss:0.21729\tvalid-auc:0.97099\n",
      "[1000]\tvalid-logloss:0.21609\tvalid-auc:0.97125\n",
      "[1200]\tvalid-logloss:0.21560\tvalid-auc:0.97134\n",
      "[1400]\tvalid-logloss:0.21463\tvalid-auc:0.97152\n",
      "[1600]\tvalid-logloss:0.21412\tvalid-auc:0.97161\n",
      "[1800]\tvalid-logloss:0.21374\tvalid-auc:0.97166\n",
      "[1945]\tvalid-logloss:0.21362\tvalid-auc:0.97167\n",
      "Fold 16 | year 2015 | AUC=0.9717  LogLoss=0.2136  Brier=0.0682  Acc=0.8985  best_iter=1895\n",
      "[0]\tvalid-logloss:0.66913\tvalid-auc:0.88715\n",
      "[200]\tvalid-logloss:0.24641\tvalid-auc:0.96292\n",
      "[400]\tvalid-logloss:0.23860\tvalid-auc:0.96488\n",
      "[600]\tvalid-logloss:0.23514\tvalid-auc:0.96573\n",
      "[800]\tvalid-logloss:0.23276\tvalid-auc:0.96634\n",
      "[1000]\tvalid-logloss:0.23148\tvalid-auc:0.96663\n",
      "[1200]\tvalid-logloss:0.23063\tvalid-auc:0.96684\n",
      "[1400]\tvalid-logloss:0.22996\tvalid-auc:0.96699\n",
      "[1600]\tvalid-logloss:0.22930\tvalid-auc:0.96713\n",
      "[1748]\tvalid-logloss:0.22912\tvalid-auc:0.96716\n",
      "Fold 17 | year 2016 | AUC=0.9672  LogLoss=0.2291  Brier=0.0735  Acc=0.8911  best_iter=1699\n",
      "[0]\tvalid-logloss:0.66938\tvalid-auc:0.88596\n",
      "[200]\tvalid-logloss:0.24463\tvalid-auc:0.96314\n",
      "[400]\tvalid-logloss:0.23642\tvalid-auc:0.96536\n",
      "[600]\tvalid-logloss:0.23227\tvalid-auc:0.96639\n",
      "[800]\tvalid-logloss:0.23042\tvalid-auc:0.96687\n",
      "[1000]\tvalid-logloss:0.22960\tvalid-auc:0.96706\n",
      "[1200]\tvalid-logloss:0.22863\tvalid-auc:0.96729\n",
      "[1400]\tvalid-logloss:0.22802\tvalid-auc:0.96746\n",
      "[1600]\tvalid-logloss:0.22737\tvalid-auc:0.96764\n",
      "[1800]\tvalid-logloss:0.22672\tvalid-auc:0.96781\n",
      "[1939]\tvalid-logloss:0.22659\tvalid-auc:0.96782\n",
      "Fold 18 | year 2017 | AUC=0.9679  LogLoss=0.2265  Brier=0.0727  Acc=0.8912  best_iter=1889\n",
      "[0]\tvalid-logloss:0.66894\tvalid-auc:0.89220\n",
      "[200]\tvalid-logloss:0.23836\tvalid-auc:0.96523\n",
      "[400]\tvalid-logloss:0.23138\tvalid-auc:0.96689\n",
      "[600]\tvalid-logloss:0.22834\tvalid-auc:0.96761\n",
      "[800]\tvalid-logloss:0.22693\tvalid-auc:0.96793\n",
      "[1000]\tvalid-logloss:0.22599\tvalid-auc:0.96814\n",
      "[1117]\tvalid-logloss:0.22572\tvalid-auc:0.96819\n",
      "Fold 19 | year 2018 | AUC=0.9682  LogLoss=0.2257  Brier=0.0722  Acc=0.8919  best_iter=1068\n",
      "[0]\tvalid-logloss:0.66694\tvalid-auc:0.92400\n",
      "[200]\tvalid-logloss:0.18012\tvalid-auc:0.98166\n",
      "[400]\tvalid-logloss:0.17474\tvalid-auc:0.98246\n",
      "[600]\tvalid-logloss:0.17232\tvalid-auc:0.98272\n",
      "[800]\tvalid-logloss:0.17072\tvalid-auc:0.98291\n",
      "[1000]\tvalid-logloss:0.16982\tvalid-auc:0.98301\n",
      "[1200]\tvalid-logloss:0.16938\tvalid-auc:0.98304\n",
      "[1400]\tvalid-logloss:0.16888\tvalid-auc:0.98308\n",
      "[1600]\tvalid-logloss:0.16843\tvalid-auc:0.98312\n",
      "[1746]\tvalid-logloss:0.16818\tvalid-auc:0.98313\n",
      "Fold 20 | year 2019 | AUC=0.9832  LogLoss=0.1681  Brier=0.0523  Acc=0.9227  best_iter=1696\n",
      "[0]\tvalid-logloss:0.66899\tvalid-auc:0.89584\n",
      "[200]\tvalid-logloss:0.22683\tvalid-auc:0.96863\n",
      "[400]\tvalid-logloss:0.22120\tvalid-auc:0.97000\n",
      "[600]\tvalid-logloss:0.21781\tvalid-auc:0.97079\n",
      "[800]\tvalid-logloss:0.21568\tvalid-auc:0.97128\n",
      "[1000]\tvalid-logloss:0.21459\tvalid-auc:0.97153\n",
      "[1200]\tvalid-logloss:0.21373\tvalid-auc:0.97170\n",
      "[1400]\tvalid-logloss:0.21276\tvalid-auc:0.97194\n",
      "[1544]\tvalid-logloss:0.21249\tvalid-auc:0.97198\n",
      "Fold 21 | year 2020 | AUC=0.9720  LogLoss=0.2125  Brier=0.0674  Acc=0.8989  best_iter=1494\n",
      "[0]\tvalid-logloss:0.67013\tvalid-auc:0.88255\n",
      "[200]\tvalid-logloss:0.26720\tvalid-auc:0.95569\n",
      "[400]\tvalid-logloss:0.25922\tvalid-auc:0.95807\n",
      "[600]\tvalid-logloss:0.25551\tvalid-auc:0.95905\n",
      "[800]\tvalid-logloss:0.25392\tvalid-auc:0.95947\n",
      "[1000]\tvalid-logloss:0.25270\tvalid-auc:0.95978\n",
      "[1200]\tvalid-logloss:0.25216\tvalid-auc:0.95992\n",
      "[1202]\tvalid-logloss:0.25214\tvalid-auc:0.95992\n",
      "Fold 22 | year 2021 | AUC=0.9599  LogLoss=0.2521  Brier=0.0816  Acc=0.8780  best_iter=1152\n",
      "[0]\tvalid-logloss:0.66940\tvalid-auc:0.88885\n",
      "[200]\tvalid-logloss:0.24313\tvalid-auc:0.96358\n",
      "[400]\tvalid-logloss:0.23543\tvalid-auc:0.96564\n",
      "[600]\tvalid-logloss:0.23185\tvalid-auc:0.96653\n",
      "[800]\tvalid-logloss:0.23071\tvalid-auc:0.96681\n",
      "[1000]\tvalid-logloss:0.22936\tvalid-auc:0.96714\n",
      "[1143]\tvalid-logloss:0.22890\tvalid-auc:0.96726\n",
      "Fold 23 | year 2022 | AUC=0.9673  LogLoss=0.2289  Brier=0.0735  Acc=0.8892  best_iter=1094\n",
      "[0]\tvalid-logloss:0.66999\tvalid-auc:0.87910\n",
      "[200]\tvalid-logloss:0.25883\tvalid-auc:0.95823\n",
      "[400]\tvalid-logloss:0.25191\tvalid-auc:0.96021\n",
      "[600]\tvalid-logloss:0.24815\tvalid-auc:0.96123\n",
      "[800]\tvalid-logloss:0.24631\tvalid-auc:0.96169\n",
      "[965]\tvalid-logloss:0.24552\tvalid-auc:0.96192\n",
      "Fold 24 | year 2023 | AUC=0.9619  LogLoss=0.2455  Brier=0.0791  Acc=0.8812  best_iter=915\n",
      "[0]\tvalid-logloss:0.67553\tvalid-auc:0.79628\n",
      "[200]\tvalid-logloss:0.39528\tvalid-auc:0.89846\n",
      "[400]\tvalid-logloss:0.38854\tvalid-auc:0.90205\n",
      "[600]\tvalid-logloss:0.38517\tvalid-auc:0.90362\n",
      "[800]\tvalid-logloss:0.38312\tvalid-auc:0.90464\n",
      "[1000]\tvalid-logloss:0.38182\tvalid-auc:0.90531\n",
      "[1154]\tvalid-logloss:0.38124\tvalid-auc:0.90566\n",
      "Fold 25 | year 2024 | AUC=0.9057  LogLoss=0.3812  Brier=0.1264  Acc=0.8091  best_iter=1105\n",
      "[0]\tvalid-logloss:0.67545\tvalid-auc:0.79713\n",
      "[200]\tvalid-logloss:0.38290\tvalid-auc:0.90476\n",
      "[400]\tvalid-logloss:0.37376\tvalid-auc:0.90943\n",
      "[600]\tvalid-logloss:0.36987\tvalid-auc:0.91128\n",
      "[800]\tvalid-logloss:0.36780\tvalid-auc:0.91220\n",
      "[1000]\tvalid-logloss:0.36701\tvalid-auc:0.91252\n",
      "[1045]\tvalid-logloss:0.36701\tvalid-auc:0.91253\n",
      "Fold 26 | year 2025 | AUC=0.9125  LogLoss=0.3670  Brier=0.1209  Acc=0.8177  best_iter=995\n",
      "\n",
      "=== CV (1 fold por año; garantía por 'id') ===\n",
      "Folds válidos: 26 / 26\n",
      "Mean AUC:      0.96375\n",
      "Mean LogLoss:  0.22444\n",
      "Mean Brier:    0.07242\n",
      "Mean Accuracy: 0.89002\n"
     ]
    }
   ],
   "source": [
    "# ==== 10) CV anual (25 folds: 2000..2025) con GARANTÍA por 'id' ====\n",
    "years_available = sorted(df[\"year\"].unique().tolist())\n",
    "fold_years = [y for y in range(2000, 2026) if y in years_available]  # 26 folds (2000–2025)\n",
    "print(f\"CV por año con garantía por 'id': {len(fold_years)} folds — años: {fold_years}\")\n",
    "\n",
    "fold_metrics, best_iters = [], []\n",
    "\n",
    "# (Opcional) Sanidad: cada id en 1 solo año\n",
    "by_id_years = df.groupby(\"id\")[\"year\"].nunique()\n",
    "if (by_id_years > 1).any():\n",
    "    print(\"Aviso: hay 'id' presentes en >1 año; aun así se agruparán por 'id' en cada fold.\")\n",
    "\n",
    "for fold, y_val in enumerate(fold_years, start=1):\n",
    "    # ids que aparecen en el año de validación\n",
    "    ids_val = df.loc[df[\"year\"] == y_val, \"id\"].unique()\n",
    "\n",
    "    # máscaras por 'id' (ambas filas del partido van juntas)\n",
    "    val_mask = df[\"id\"].isin(ids_val)\n",
    "    train_mask = ~val_mask\n",
    "\n",
    "    # split\n",
    "    X_tr_raw, X_va_raw = X.loc[train_mask], X.loc[val_mask]\n",
    "    y_tr, y_va = y[train_mask], y[val_mask]\n",
    "\n",
    "    # preprocesamiento sin fuga\n",
    "    pre_fold = clone(pre_template)\n",
    "    pre_fold.fit(X_tr_raw)\n",
    "    Xtr = pre_fold.transform(X_tr_raw)\n",
    "    Xva = pre_fold.transform(X_va_raw)\n",
    "\n",
    "\n",
    "    if not sparse.isspmatrix_csr(Xtr):\n",
    "        Xtr = sparse.csr_matrix(Xtr)\n",
    "    if not sparse.isspmatrix_csr(Xva):\n",
    "        Xva = sparse.csr_matrix(Xva)\n",
    "    \n",
    "    Xtr = Xtr.astype(np.float32)\n",
    "    Xva = Xva.astype(np.float32)\n",
    "\n",
    "\n",
    "    # entrenamiento con xgboost.train (+ early stopping) — versión ligera\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"auc\"],\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eta\": 0.05,              # antes 0.05\n",
    "        \"max_depth\": 6,           # antes 7\n",
    "        \"min_child_weight\": 12,   # antes 8\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.65, # antes 0.7\n",
    "        \"colsample_bynode\": 0.8,\n",
    "        \"gamma\": 2.0,             # antes 1.0\n",
    "        \"lambda\": 1.5,            # antes 1.0\n",
    "        \"alpha\": 0.8,             # antes 0.5\n",
    "        \"nthread\": 8,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    dtrain = xgb.DMatrix(Xtr, label=y_tr)\n",
    "    dvalid = xgb.DMatrix(Xva, label=y_va)\n",
    "    es = XGBEarlyStopping(rounds=50, save_best=True, maximize=False, metric_name=\"logloss\")  # antes 100\n",
    "    booster = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=2000,     # antes 4000\n",
    "        evals=[(dvalid, \"valid\")],\n",
    "        callbacks=[es],\n",
    "        verbose_eval=200\n",
    "    )\n",
    "\n",
    "\n",
    "    # métricas\n",
    "    if y_va.size and len(np.unique(y_va)) > 1:\n",
    "        p_va = booster.predict(dvalid)\n",
    "        auc = float(roc_auc_score(y_va, p_va))\n",
    "        ll  = float(log_loss(y_va, p_va))\n",
    "        br  = float(brier_score_loss(y_va, p_va))\n",
    "        acc = float(((p_va >= 0.5) == y_va).mean())   # accuracy con umbral 0.5\n",
    "    else:\n",
    "        auc = ll = br = acc = np.nan\n",
    "\n",
    "    # mejor iteración (robusto)\n",
    "    best_iter = getattr(booster, \"best_iteration\", None)\n",
    "    if best_iter is None:\n",
    "        best_iter = getattr(booster, \"best_ntree_limit\", None)\n",
    "        if best_iter is None:\n",
    "            best_iter = 4000\n",
    "    best_iter = int(best_iter)\n",
    "\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold,\n",
    "        \"year_valid\": int(y_val),\n",
    "        \"AUC\": auc, \"LogLoss\": ll, \"Brier\": br, \"Accuracy\": acc,\n",
    "        \"best_iteration\": best_iter\n",
    "    })\n",
    "    best_iters.append(best_iter)\n",
    "\n",
    "    del booster, dtrain, dvalid, Xtr, Xva, pre_fold, X_tr_raw, X_va_raw, y_tr, y_va\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Fold {fold} | year {y_val} | \"\n",
    "          f\"AUC={auc:.4f}  LogLoss={ll:.4f}  Brier={br:.4f}  Acc={acc:.4f}  \"\n",
    "          f\"best_iter={best_iter}\")\n",
    "\n",
    "# ==== 11) Resumen CV ====\n",
    "valid_aucs   = [m[\"AUC\"] for m in fold_metrics if not np.isnan(m[\"AUC\"])]\n",
    "valid_lls    = [m[\"LogLoss\"] for m in fold_metrics if not np.isnan(m[\"LogLoss\"])]\n",
    "valid_briers = [m[\"Brier\"] for m in fold_metrics if not np.isnan(m[\"Brier\"])]\n",
    "valid_accs   = [m[\"Accuracy\"] for m in fold_metrics if not np.isnan(m[\"Accuracy\"])]\n",
    "\n",
    "print(\"\\n=== CV (1 fold por año; garantía por 'id') ===\")\n",
    "print(f\"Folds válidos: {len(valid_aucs)} / {len(fold_years)}\")\n",
    "print(f\"Mean AUC:      {np.mean(valid_aucs):.5f}\"  if valid_aucs else \"Mean AUC: n/a\")\n",
    "print(f\"Mean LogLoss:  {np.mean(valid_lls):.5f}\"   if valid_lls else \"Mean LogLoss: n/a\")\n",
    "print(f\"Mean Brier:    {np.mean(valid_briers):.5f}\"if valid_briers else \"Mean Brier: n/a\")\n",
    "print(f\"Mean Accuracy: {np.mean(valid_accs):.5f}\"  if valid_accs else \"Mean Accuracy: n/a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc5f7c1-5d0f-4610-9794-383037daacef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Construyendo OOF para calibración en 23 años (2000–2022)...\n",
      "OOF fold 1/23 (año 2000) listo\n",
      "OOF fold 2/23 (año 2001) listo\n",
      "OOF fold 3/23 (año 2002) listo\n",
      "OOF fold 4/23 (año 2003) listo\n",
      "OOF fold 5/23 (año 2004) listo\n",
      "OOF fold 6/23 (año 2005) listo\n",
      "OOF fold 7/23 (año 2006) listo\n",
      "OOF fold 8/23 (año 2007) listo\n",
      "OOF fold 9/23 (año 2008) listo\n",
      "OOF fold 10/23 (año 2009) listo\n",
      "OOF fold 11/23 (año 2010) listo\n",
      "OOF fold 12/23 (año 2011) listo\n",
      "OOF fold 13/23 (año 2012) listo\n",
      "OOF fold 14/23 (año 2013) listo\n",
      "OOF fold 15/23 (año 2014) listo\n",
      "OOF fold 16/23 (año 2015) listo\n",
      "OOF fold 17/23 (año 2016) listo\n",
      "OOF fold 18/23 (año 2017) listo\n",
      "OOF fold 19/23 (año 2018) listo\n",
      "OOF fold 20/23 (año 2019) listo\n",
      "OOF fold 21/23 (año 2020) listo\n",
      "OOF fold 22/23 (año 2021) listo\n",
      "OOF fold 23/23 (año 2022) listo\n",
      "\n",
      "OOF sin calib:  AUC 0.9719 | LogLoss 0.2113 | Brier 0.0679 | Acc@0.5 0.8974\n",
      "OOF calibrado:  AUC 0.9719 | LogLoss 0.2099 | Brier 0.0678 | Acc@0.5 0.8975\n",
      "Umbral óptimo por costes -> thr* = 0.4959 | coste medio 0.10242\n"
     ]
    }
   ],
   "source": [
    "# ==== 12) OOF 2000–2022: calibración isotónica + umbral por costes ====\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def find_optimal_threshold(y_true, prob, cost_fp=1.0, cost_fn=1.0):\n",
    "    y = np.asarray(y_true, dtype=int)\n",
    "    p = np.asarray(prob, dtype=float)\n",
    "    order = np.argsort(-p)  # desc\n",
    "    p_sorted = p[order]\n",
    "    y_sorted = y[order]\n",
    "    pos = (y_sorted == 1).astype(int)\n",
    "    neg = 1 - pos\n",
    "    pref_pos = np.concatenate([[0], np.cumsum(pos)])\n",
    "    pref_neg = np.concatenate([[0], np.cumsum(neg)])\n",
    "    total_pos = int(pref_pos[-1])\n",
    "    i = np.arange(0, len(p_sorted) + 1)  # nº predicciones positivas\n",
    "    FP = pref_neg[i]\n",
    "    FN = total_pos - pref_pos[i]\n",
    "    cost = cost_fp * FP + cost_fn * FN\n",
    "    i_best = int(np.argmin(cost))\n",
    "    if i_best == 0:\n",
    "        thr = 1.0 + 1e-12\n",
    "    elif i_best == len(p_sorted):\n",
    "        thr = 0.0\n",
    "    else:\n",
    "        thr = (p_sorted[i_best - 1] + p_sorted[i_best]) / 2.0\n",
    "    return float(thr), float(cost[i_best] / len(y))\n",
    "\n",
    "years_all = sorted(df[\"year\"].unique())\n",
    "oof_years = [yy for yy in years_all if 2000 <= yy <= 2022]\n",
    "oof_pred = np.full(df.shape[0], np.nan, dtype=np.float32)\n",
    "mask_pool = df[\"year\"].isin(oof_years).values\n",
    "\n",
    "print(f\"\\nConstruyendo OOF para calibración en {len(oof_years)} años (2000–2022)...\")\n",
    "for k, y_val in enumerate(oof_years, start=1):\n",
    "    ids_val = df.loc[df[\"year\"] == y_val, \"id\"].unique()\n",
    "    val_mask = df[\"id\"].isin(ids_val).values\n",
    "    train_mask = mask_pool & (~val_mask)\n",
    "\n",
    "    X_tr_raw, X_va_raw = X.loc[train_mask], X.loc[val_mask]\n",
    "    y_tr, y_va = y[train_mask], y[val_mask]\n",
    "\n",
    "    pre_fold = clone(pre_template)\n",
    "    pre_fold.fit(X_tr_raw)\n",
    "    Xtr = pre_fold.transform(X_tr_raw)\n",
    "    Xva = pre_fold.transform(X_va_raw)\n",
    "\n",
    "    if not sparse.isspmatrix_csr(Xtr):\n",
    "        Xtr = sparse.csr_matrix(Xtr)\n",
    "    if not sparse.isspmatrix_csr(Xva):\n",
    "        Xva = sparse.csr_matrix(Xva)\n",
    "    Xtr = Xtr.astype(np.float32)\n",
    "    Xva = Xva.astype(np.float32)\n",
    "\n",
    "    params_oof = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"auc\"],\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eta\": 0.10, \"max_depth\": 6, \"min_child_weight\": 12,\n",
    "        \"subsample\": 0.8, \"colsample_bytree\": 0.65, \"colsample_bynode\": 0.8,\n",
    "        \"gamma\": 2.0, \"lambda\": 1.5, \"alpha\": 0.8,\n",
    "        \"nthread\": 8, \"seed\": 42\n",
    "    }\n",
    "    dtr = xgb.DMatrix(Xtr, label=y_tr)\n",
    "    dva = xgb.DMatrix(Xva, label=y_va)\n",
    "    es_oof = XGBEarlyStopping(rounds=50, save_best=True, maximize=False, metric_name=\"logloss\")\n",
    "    booster = xgb.train(\n",
    "        params=params_oof,\n",
    "        dtrain=dtr,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dva, \"valid\")],\n",
    "        callbacks=[es_oof],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    oof_pred[val_mask] = booster.predict(dva).astype(np.float32)\n",
    "\n",
    "    del booster, dtr, dva, pre_fold, Xtr, Xva, X_tr_raw, X_va_raw, y_tr, y_va\n",
    "    gc.collect()\n",
    "    print(f\"OOF fold {k}/{len(oof_years)} (año {y_val}) listo\")\n",
    "\n",
    "idx = np.where(mask_pool & ~np.isnan(oof_pred))[0]\n",
    "y_oof = y[idx]\n",
    "p_oof = oof_pred[idx]\n",
    "\n",
    "# Calibración isotónica\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(p_oof, y_oof)\n",
    "p_oof_cal = iso.predict(p_oof)\n",
    "\n",
    "# Métricas OOF (sin y con calibración)\n",
    "def _m(ytrue, p):\n",
    "    return (\n",
    "        float(roc_auc_score(ytrue, p)),\n",
    "        float(log_loss(ytrue, p)),\n",
    "        float(brier_score_loss(ytrue, p)),\n",
    "        float(((p >= 0.5).astype(int) == ytrue).mean())\n",
    "    )\n",
    "\n",
    "auc_r, ll_r, br_r, acc_r = _m(y_oof, p_oof)\n",
    "auc_c, ll_c, br_c, acc_c = _m(y_oof, p_oof_cal)\n",
    "print(f\"\\nOOF sin calib:  AUC {auc_r:.4f} | LogLoss {ll_r:.4f} | Brier {br_r:.4f} | Acc@0.5 {acc_r:.4f}\")\n",
    "print(f\"OOF calibrado:  AUC {auc_c:.4f} | LogLoss {ll_c:.4f} | Brier {br_c:.4f} | Acc@0.5 {acc_c:.4f}\")\n",
    "\n",
    "# Umbral por costes (inicialmente cost_fp=1, cost_fn=1)\n",
    "thr_star, cost_star = find_optimal_threshold(y_oof, p_oof_cal, cost_fp=1.0, cost_fn=1.0)\n",
    "print(f\"Umbral óptimo por costes -> thr* = {thr_star:.4f} | coste medio {cost_star:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "862e4bd7-e2bb-4794-ba68-7911f27f8cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.64551\tvalid-auc:0.89403\n",
      "[100]\tvalid-logloss:0.24373\tvalid-auc:0.96321\n",
      "[200]\tvalid-logloss:0.23502\tvalid-auc:0.96556\n",
      "[300]\tvalid-logloss:0.23250\tvalid-auc:0.96615\n",
      "[400]\tvalid-logloss:0.23155\tvalid-auc:0.96640\n",
      "[500]\tvalid-logloss:0.23059\tvalid-auc:0.96664\n",
      "[600]\tvalid-logloss:0.23020\tvalid-auc:0.96665\n",
      "[700]\tvalid-logloss:0.22961\tvalid-auc:0.96675\n",
      "[800]\tvalid-logloss:0.22925\tvalid-auc:0.96682\n",
      "[836]\tvalid-logloss:0.22945\tvalid-auc:0.96675\n",
      "\n",
      "[Hold-out] best_iter=786\n",
      "\n",
      "=== Test 2023–2025 (probabilidades calibradas) ===\n",
      "Año 2023 | AUC 0.9626 | LogLoss 0.2421 | Brier 0.0783 | Acc@0.5 0.8803 | Acc@0.496 0.8802\n",
      "Año 2024 | AUC 0.8895 | LogLoss 0.4475 | Brier 0.1413 | Acc@0.5 0.7871 | Acc@0.496 0.7881\n",
      "Año 2025 | AUC 0.8915 | LogLoss 0.4485 | Brier 0.1399 | Acc@0.5 0.7940 | Acc@0.496 0.7934\n",
      "\n",
      "Media 2023–2025 | AUC 0.9145 | LogLoss 0.3794 | Brier 0.1198 | Acc@0.5 0.8205 | Acc@0.496 0.8206\n"
     ]
    }
   ],
   "source": [
    "#==== 13) ==== Entrenamiento final con hold-out (train<=2022, test 2023–2025) — versión eficiente ====\n",
    "mask_tr_es     = df[\"year\"] <= 2021\n",
    "mask_va_es     = df[\"year\"] == 2022\n",
    "mask_fit_final = df[\"year\"] <= 2022\n",
    "mask_test      = (df[\"year\"] >= 2023) & (df[\"year\"] <= 2025)\n",
    "\n",
    "# --- Early stopping en 2022 (preprocesador SOLO con <=2021, para no fugar) ---\n",
    "pre_es  = clone(pre_template)\n",
    "Xtr_es  = pre_es.fit_transform(X.loc[mask_tr_es])   # 1 pasada\n",
    "Xva_es  = pre_es.transform(X.loc[mask_va_es])\n",
    "\n",
    "if not sparse.isspmatrix_csr(Xtr_es): Xtr_es = sparse.csr_matrix(Xtr_es)\n",
    "if not sparse.isspmatrix_csr(Xva_es): Xva_es = sparse.csr_matrix(Xva_es)\n",
    "Xtr_es = Xtr_es.astype(np.float32)\n",
    "Xva_es = Xva_es.astype(np.float32)\n",
    "\n",
    "params_es = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"auc\"],\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eta\": 0.10, \"max_depth\": 6, \"min_child_weight\": 12,\n",
    "    \"subsample\": 0.8, \"colsample_bytree\": 0.65, \"colsample_bynode\": 0.8,\n",
    "    \"gamma\": 2.0, \"lambda\": 1.5, \"alpha\": 0.8,\n",
    "    \"nthread\": 8, \"seed\": 42\n",
    "}\n",
    "dtr_es = xgb.DMatrix(Xtr_es, label=y[mask_tr_es])\n",
    "dva_es = xgb.DMatrix(Xva_es, label=y[mask_va_es])\n",
    "es_cb  = XGBEarlyStopping(rounds=50, save_best=True, maximize=False, metric_name=\"logloss\")\n",
    "\n",
    "booster_es = xgb.train(\n",
    "    params_es, dtr_es, num_boost_round=2000,\n",
    "    evals=[(dva_es, \"valid\")], callbacks=[es_cb], verbose_eval=100\n",
    ")\n",
    "\n",
    "best_iter = getattr(booster_es, \"best_iteration\", None)\n",
    "if best_iter is None:\n",
    "    best_iter = getattr(booster_es, \"best_ntree_limit\", 1000)\n",
    "best_iter = int(best_iter)\n",
    "print(f\"\\n[Hold-out] best_iter={best_iter}\")\n",
    "\n",
    "# Limpieza temprana\n",
    "del pre_es, Xtr_es, Xva_es, dtr_es, dva_es, booster_es\n",
    "gc.collect()\n",
    "\n",
    "# --- Reentrenar final con <=2022 usando best_iter (nuevo preprocesador con <=2022) ---\n",
    "pre_final = clone(pre_template)\n",
    "X_final   = pre_final.fit_transform(X.loc[mask_fit_final])  # 1 pasada\n",
    "\n",
    "if not sparse.isspmatrix_csr(X_final): X_final = sparse.csr_matrix(X_final)\n",
    "X_final = X_final.astype(np.float32)\n",
    "\n",
    "d_final = xgb.DMatrix(X_final, label=y[mask_fit_final])\n",
    "booster_final = xgb.train(params_es, d_final, num_boost_round=best_iter, evals=[], verbose_eval=False)\n",
    "\n",
    "# Limpieza intermedia\n",
    "del X_final, d_final\n",
    "gc.collect()\n",
    "\n",
    "# --- TEST 2023–2025: transformar y predecir en UN SOLO BATCH ---\n",
    "if mask_test.any():\n",
    "    Xt_test = pre_final.transform(X.loc[mask_test])\n",
    "    if not sparse.isspmatrix_csr(Xt_test): Xt_test = sparse.csr_matrix(Xt_test)\n",
    "    Xt_test = Xt_test.astype(np.float32)\n",
    "\n",
    "    dtest = xgb.DMatrix(Xt_test)\n",
    "    p_raw_test = booster_final.predict(dtest)\n",
    "    p_cal_test = iso.predict(p_raw_test)  # calibración OOF 2000–2022\n",
    "    y_test     = y[mask_test]\n",
    "    years_test = df.loc[mask_test, \"year\"].astype(int).values\n",
    "\n",
    "    # métricas por año sin repetir transform/predict\n",
    "    print(\"\\n=== Test 2023–2025 (probabilidades calibradas) ===\")\n",
    "    mean_vals = {\"auc\": [], \"ll\": [], \"br\": [], \"acc05\": [], \"accst\": []}\n",
    "    for yr in (2023, 2024, 2025):\n",
    "        m = (years_test == yr)\n",
    "        if not np.any(m): \n",
    "            continue\n",
    "        y_true = y_test[m]\n",
    "        p      = p_cal_test[m]\n",
    "        auc = float(roc_auc_score(y_true, p))\n",
    "        ll  = float(log_loss(y_true, p))\n",
    "        br  = float(brier_score_loss(y_true, p))\n",
    "        acc05 = float(((p >= 0.5).astype(int) == y_true).mean())\n",
    "        accst = float(((p >= thr_star).astype(int) == y_true).mean())\n",
    "        print(f\"Año {yr} | AUC {auc:.4f} | LogLoss {ll:.4f} | Brier {br:.4f} | \"\n",
    "              f\"Acc@0.5 {acc05:.4f} | Acc@{thr_star:.3f} {accst:.4f}\")\n",
    "        mean_vals[\"auc\"].append(auc); mean_vals[\"ll\"].append(ll); mean_vals[\"br\"].append(br)\n",
    "        mean_vals[\"acc05\"].append(acc05); mean_vals[\"accst\"].append(accst)\n",
    "\n",
    "    if mean_vals[\"auc\"]:\n",
    "        from statistics import mean\n",
    "        print(f\"\\nMedia 2023–2025 | AUC {mean(mean_vals['auc']):.4f} | \"\n",
    "              f\"LogLoss {mean(mean_vals['ll']):.4f} | Brier {mean(mean_vals['br']):.4f} | \"\n",
    "              f\"Acc@0.5 {mean(mean_vals['acc05']):.4f} | Acc@{thr_star:.3f} {mean(mean_vals['accst']):.4f}\")\n",
    "\n",
    "    # limpieza test\n",
    "    del Xt_test, dtest, p_raw_test, p_cal_test, y_test, years_test\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"\\nNo hay filas en 2023–2025 en este dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "616c71f7-7b51-49d7-9f90-2ebca184ac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.66215\tvalid-auc:0.94571\n",
      "[100]\tvalid-logloss:0.25757\tvalid-auc:0.96049\n",
      "[200]\tvalid-logloss:0.24299\tvalid-auc:0.96343\n",
      "[300]\tvalid-logloss:0.23839\tvalid-auc:0.96467\n",
      "[400]\tvalid-logloss:0.23515\tvalid-auc:0.96554\n",
      "[500]\tvalid-logloss:0.23347\tvalid-auc:0.96601\n",
      "[600]\tvalid-logloss:0.23181\tvalid-auc:0.96640\n",
      "[700]\tvalid-logloss:0.23102\tvalid-auc:0.96662\n",
      "[800]\tvalid-logloss:0.23020\tvalid-auc:0.96683\n",
      "[900]\tvalid-logloss:0.22968\tvalid-auc:0.96694\n",
      "[1000]\tvalid-logloss:0.22945\tvalid-auc:0.96698\n",
      "[1100]\tvalid-logloss:0.22913\tvalid-auc:0.96705\n",
      "[1200]\tvalid-logloss:0.22912\tvalid-auc:0.96705\n",
      "[1300]\tvalid-logloss:0.22879\tvalid-auc:0.96711\n",
      "[1365]\tvalid-logloss:0.22864\tvalid-auc:0.96713\n",
      "[Hold-out selector] best_iter=1315\n",
      "\n",
      "=== Hold-out 2023–2025 por tipo de torneo (umbral 0.5) ===\n",
      "Challenger-like | n=54682 | AUC 0.9200 | LogLoss 0.3696 | Brier 0.1177 | Acc@0.5 0.8228\n",
      "   - Año 2023 | n=18968 | AUC 0.9638 | LogLoss 0.2380 | Brier 0.0772 | Acc@0.5 0.8831\n",
      "   - Año 2024 | n=20106 | AUC 0.8942 | LogLoss 0.4335 | Brier 0.1376 | Acc@0.5 0.7916\n",
      "   - Año 2025 | n=15608 | AUC 0.8895 | LogLoss 0.4474 | Brier 0.1414 | Acc@0.5 0.7898\n",
      "ATP Tour | n=11694 | AUC 0.9142 | LogLoss 0.3738 | Brier 0.1207 | Acc@0.5 0.8187\n",
      "   - Año 2023 | n=4250 | AUC 0.9461 | LogLoss 0.2885 | Brier 0.0936 | Acc@0.5 0.8581\n",
      "   - Año 2024 | n=4314 | AUC 0.8875 | LogLoss 0.4321 | Brier 0.1408 | Acc@0.5 0.7911\n",
      "   - Año 2025 | n=3130 | AUC 0.9026 | LogLoss 0.4093 | Brier 0.1300 | Acc@0.5 0.8032\n",
      "Masters 1000 | n=5522 | AUC 0.8971 | LogLoss 0.4313 | Brier 0.1352 | Acc@0.5 0.7997\n",
      "   - Año 2023 | n=1920 | AUC 0.9583 | LogLoss 0.2587 | Brier 0.0823 | Acc@0.5 0.8812\n",
      "   - Año 2024 | n=1918 | AUC 0.8433 | LogLoss 0.5636 | Brier 0.1757 | Acc@0.5 0.7435\n",
      "   - Año 2025 | n=1684 | AUC 0.8751 | LogLoss 0.4774 | Brier 0.1495 | Acc@0.5 0.7708\n",
      "Grand Slams | n=5844 | AUC 0.9324 | LogLoss 0.3682 | Brier 0.1091 | Acc@0.5 0.8443\n",
      "   - Año 2023 | n=1912 | AUC 0.9878 | LogLoss 0.1427 | Brier 0.0435 | Acc@0.5 0.9378\n",
      "   - Año 2024 | n=2012 | AUC 0.8869 | LogLoss 0.4918 | Brier 0.1478 | Acc@0.5 0.7863\n",
      "   - Año 2025 | n=1920 | AUC 0.9026 | LogLoss 0.4632 | Brier 0.1339 | Acc@0.5 0.8120\n"
     ]
    }
   ],
   "source": [
    "# === Hold-out 2023–2025 por tipo de torneo (train <=2022), umbral 0.5 ===\n",
    "\n",
    "# 1) Máscaras temporales\n",
    "mask_tr_es     = df[\"year\"] <= 2021\n",
    "mask_va_es     = df[\"year\"] == 2022\n",
    "mask_fit_final = df[\"year\"] <= 2022\n",
    "mask_test      = (df[\"year\"] >= 2023) & (df[\"year\"] <= 2025)\n",
    "\n",
    "# 2) Features sin 'id' por seguridad\n",
    "X_use = X.drop(columns=[\"id\"], errors=\"ignore\")\n",
    "\n",
    "# 3) Early stopping en 2022 para elegir best_iter\n",
    "pre_es = clone(pre_template)\n",
    "Xtr_es = pre_es.fit_transform(X_use.loc[mask_tr_es])\n",
    "Xva_es = pre_es.transform(X_use.loc[mask_va_es])\n",
    "\n",
    "if not sparse.isspmatrix_csr(Xtr_es): Xtr_es = sparse.csr_matrix(Xtr_es)\n",
    "if not sparse.isspmatrix_csr(Xva_es): Xva_es = sparse.csr_matrix(Xva_es)\n",
    "Xtr_es = Xtr_es.astype(np.float32); Xva_es = Xva_es.astype(np.float32)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"auc\"],\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eta\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 12,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.65,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"gamma\": 2.0,\n",
    "    \"lambda\": 1.5,\n",
    "    \"alpha\": 0.8,\n",
    "    \"nthread\": 8,\n",
    "    \"seed\": 42\n",
    "}\n",
    "dtr_es = xgb.DMatrix(Xtr_es, label=y[mask_tr_es])\n",
    "dva_es = xgb.DMatrix(Xva_es, label=y[mask_va_es])\n",
    "\n",
    "es_cb = XGBEarlyStopping(rounds=50, save_best=True, maximize=False, metric_name=\"logloss\")\n",
    "booster_es = xgb.train(params, dtr_es, num_boost_round=2000, evals=[(dva_es, \"valid\")], callbacks=[es_cb], verbose_eval=100)\n",
    "\n",
    "best_iter = getattr(booster_es, \"best_iteration\", None)\n",
    "if best_iter is None:\n",
    "    best_iter = getattr(booster_es, \"best_ntree_limit\", 1000)\n",
    "best_iter = int(best_iter)\n",
    "print(f\"[Hold-out selector] best_iter={best_iter}\")\n",
    "\n",
    "# Limpieza\n",
    "del pre_es, Xtr_es, Xva_es, dtr_es, dva_es, booster_es\n",
    "gc.collect()\n",
    "\n",
    "# 4) Reentrenar final con <=2022 usando best_iter\n",
    "pre_final = clone(pre_template)\n",
    "X_final = pre_final.fit_transform(X_use.loc[mask_fit_final])\n",
    "if not sparse.isspmatrix_csr(X_final): X_final = sparse.csr_matrix(X_final)\n",
    "X_final = X_final.astype(np.float32)\n",
    "\n",
    "d_final = xgb.DMatrix(X_final, label=y[mask_fit_final])\n",
    "booster_final = xgb.train(params, d_final, num_boost_round=best_iter, evals=[], verbose_eval=False)\n",
    "\n",
    "del X_final, d_final\n",
    "gc.collect()\n",
    "\n",
    "# 5) Transformar y predecir TODO 2023–2025 de una vez\n",
    "if mask_test.any():\n",
    "    Xt_test = pre_final.transform(X_use.loc[mask_test])\n",
    "    if not sparse.isspmatrix_csr(Xt_test): Xt_test = sparse.csr_matrix(Xt_test)\n",
    "    Xt_test = Xt_test.astype(np.float32)\n",
    "    dtest   = xgb.DMatrix(Xt_test)\n",
    "    p_raw   = booster_final.predict(dtest)\n",
    "\n",
    "    # Usa calibración si 'iso' existe; si no, usa probas crudas\n",
    "    try:\n",
    "        p_test = iso.predict(p_raw)\n",
    "    except NameError:\n",
    "        p_test = p_raw\n",
    "\n",
    "    y_test     = y[mask_test]\n",
    "    years_test = df.loc[mask_test, \"year\"].astype(int).values\n",
    "\n",
    "    # Serie segura para operaciones de texto (maneja NA)\n",
    "    tc_s = (\n",
    "        df.loc[mask_test, \"tournament_category\"]\n",
    "          .astype(\"string\")\n",
    "          .str.strip()\n",
    "          .str.lower()\n",
    "    )\n",
    "\n",
    "    def mask_eq_or_starts(s: pd.Series, token: str) -> pd.Series:\n",
    "        tok = token.lower()\n",
    "        return s.eq(tok) | s.str.startswith(tok, na=False)\n",
    "\n",
    "    groups = [\n",
    "        (\"ch\",   \"Challenger-like\"),\n",
    "        (\"atp\",  \"ATP Tour\"),\n",
    "        (\"1000\", \"Masters 1000\"),\n",
    "        (\"gs\",   \"Grand Slams\"),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n=== Hold-out 2023–2025 por tipo de torneo (umbral 0.5) ===\")\n",
    "    for token, label in groups:\n",
    "        m = mask_eq_or_starts(tc_s, token).to_numpy()\n",
    "        if not np.any(m):\n",
    "            print(f\"{label}: sin datos en 2023–2025.\")\n",
    "            continue\n",
    "\n",
    "        y_g = y_test[m]\n",
    "        p_g = p_test[m]\n",
    "        auc = float(roc_auc_score(y_g, p_g)) if len(np.unique(y_g)) > 1 else np.nan\n",
    "        ll  = float(log_loss(y_g, p_g))\n",
    "        br  = float(brier_score_loss(y_g, p_g))\n",
    "        acc = float(((p_g >= 0.5).astype(int) == y_g).mean())\n",
    "        print(f\"{label} | n={m.sum()} | AUC {auc:.4f} | LogLoss {ll:.4f} | Brier {br:.4f} | Acc@0.5 {acc:.4f}\")\n",
    "\n",
    "        # (Opcional) métricas por año dentro del grupo\n",
    "        for yr in (2023, 2024, 2025):\n",
    "            my = m & (years_test == yr)\n",
    "            if not np.any(my):\n",
    "                continue\n",
    "            yg, pg = y_test[my], p_test[my]\n",
    "            aucy = float(roc_auc_score(yg, pg)) if len(np.unique(yg)) > 1 else np.nan\n",
    "            lly  = float(log_loss(yg, pg))\n",
    "            bry  = float(brier_score_loss(yg, pg))\n",
    "            accy = float(((pg >= 0.5).astype(int) == yg).mean())\n",
    "            print(f\"   - Año {yr} | n={my.sum()} | AUC {aucy:.4f} | LogLoss {lly:.4f} | Brier {bry:.4f} | Acc@0.5 {accy:.4f}\")\n",
    "\n",
    "    # limpieza test\n",
    "    del Xt_test, dtest, p_raw, p_test, y_test, years_test, tc_s\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"No hay filas en 2023–2025.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1a490-e057-49f7-a188-aff6a47f6e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
